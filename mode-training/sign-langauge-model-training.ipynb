{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10366272,"sourceType":"datasetVersion","datasetId":6420653}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install XlsxWriter\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T03:58:28.181436Z","iopub.execute_input":"2025-01-04T03:58:28.181725Z","iopub.status.idle":"2025-01-04T03:58:33.282744Z","shell.execute_reply.started":"2025-01-04T03:58:28.181694Z","shell.execute_reply":"2025-01-04T03:58:33.281890Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting XlsxWriter\n  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\nDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: XlsxWriter\nSuccessfully installed XlsxWriter-3.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport ast\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport os\nfrom datetime import datetime\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T03:58:33.284238Z","iopub.execute_input":"2025-01-04T03:58:33.284592Z","iopub.status.idle":"2025-01-04T03:58:33.290895Z","shell.execute_reply.started":"2025-01-04T03:58:33.284558Z","shell.execute_reply":"2025-01-04T03:58:33.290068Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sign-langauge-translater/combined_dataset.csv')\n# df2 = pd.read_csv('hand_landmarks_from_4500_to_5000.csv')\n\n# df = pd.concat([df1, df2], ignore_index=True)\n# df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T03:58:33.292712Z","iopub.execute_input":"2025-01-04T03:58:33.292920Z","iopub.status.idle":"2025-01-04T03:58:36.785331Z","shell.execute_reply.started":"2025-01-04T03:58:33.292902Z","shell.execute_reply":"2025-01-04T03:58:36.784615Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T03:58:36.786612Z","iopub.execute_input":"2025-01-04T03:58:36.786907Z","iopub.status.idle":"2025-01-04T03:58:36.803115Z","shell.execute_reply.started":"2025-01-04T03:58:36.786877Z","shell.execute_reply":"2025-01-04T03:58:36.802472Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"  class                                          landmarks\n0     a  [[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...\n1     a  [[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...\n2     a  [[[[0.5544530153274536, 0.8488561511039734, -6...\n3     a  [[[[0.558426558971405, 0.804563581943512, -5.1...\n4     a  [[[[0.5819562673568726, 0.8947145342826843, -6...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>landmarks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a</td>\n      <td>[[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a</td>\n      <td>[[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a</td>\n      <td>[[[[0.5544530153274536, 0.8488561511039734, -6...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>a</td>\n      <td>[[[[0.558426558971405, 0.804563581943512, -5.1...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a</td>\n      <td>[[[[0.5819562673568726, 0.8947145342826843, -6...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def preprocess_data(df, landmarks_column=\"landmarks\", class_column=\"class\"):\n    \"\"\"\n    Preprocess the dataset for model training.\n\n    Args:\n        df (pd.DataFrame): DataFrame with class labels and landmarks columns.\n        landmarks_column (str): Name of the column containing landmarks.\n        class_column (str): Name of the column containing class labels.\n\n    Returns:\n        pd.DataFrame: Processed DataFrame with numeric landmarks and encoded labels.\n        LabelEncoder: Encoder for decoding class labels.\n    \"\"\"\n    # Ensure landmarks are in numeric format\n    df[landmarks_column] = df[landmarks_column].apply(\n        lambda x: np.array(ast.literal_eval(x)) if isinstance(x, str) else np.array(x)\n    )\n\n    # Encode class labels\n    le = LabelEncoder()\n    df[\"encoded_class\"] = le.fit_transform(df[class_column])\n\n    return df, le\n\n# Apply the preprocessing\ndf, label_encoder = preprocess_data(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T03:58:36.804034Z","iopub.execute_input":"2025-01-04T03:58:36.804296Z","iopub.status.idle":"2025-01-04T03:59:41.354727Z","shell.execute_reply.started":"2025-01-04T03:58:36.804253Z","shell.execute_reply":"2025-01-04T03:59:41.353825Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"num_classes = len(label_encoder.classes_)\nprint(label_encoder.classes_, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T03:59:41.355650Z","iopub.execute_input":"2025-01-04T03:59:41.355967Z","iopub.status.idle":"2025-01-04T03:59:41.361194Z","shell.execute_reply.started":"2025-01-04T03:59:41.355935Z","shell.execute_reply":"2025-01-04T03:59:41.360467Z"}},"outputs":[{"name":"stdout","text":"['a' 'about' 'aim' 'all' 'and' 'audio' 'b' 'barrier' 'break' 'c' 'can'\n 'communication' 'creative' 'd' 'detect' 'developed' 'e' 'f' 'g' 'h'\n 'have' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'our' 'p' 'project' 'q' 'r' 's'\n 'sign language' 'solution' 't' 'team' 'text' 'that' 'to' 'translate' 'u'\n 'v' 'w' 'what' 'x' 'y' 'you' 'z'] 50\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def augment_landmarks(landmarks, max_noise=0.01, scale_range=(0.9, 1.1)):\n    \"\"\"\n    Apply random augmentations to landmarks.\n\n    Args:\n        landmarks (np.ndarray): Original landmarks array.\n        max_noise (float): Maximum noise to add to each coordinate.\n        scale_range (tuple): Range for random scaling factors.\n\n    Returns:\n        np.ndarray: Augmented landmarks.\n    \"\"\"\n    # Ensure landmarks are float64 for operations\n    landmarks = landmarks.astype(np.float64)\n    \n    # Add random noise\n    noise = np.random.uniform(-max_noise, max_noise, size=landmarks.shape)\n    landmarks += noise\n\n    # Apply random scaling\n    scale_factor = random.uniform(*scale_range)\n    landmarks *= scale_factor\n\n    return landmarks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T03:59:41.361905Z","iopub.execute_input":"2025-01-04T03:59:41.362094Z","iopub.status.idle":"2025-01-04T03:59:41.376099Z","shell.execute_reply.started":"2025-01-04T03:59:41.362077Z","shell.execute_reply":"2025-01-04T03:59:41.375342Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def augment_dataset_in_batches(df, num_augmentations=5, batch_size=1000):\n    \"\"\"\n    Augment the dataset in batches to avoid memory overflow.\n\n    Args:\n        df (pd.DataFrame): DataFrame with 'class' and 'landmarks' columns.\n        num_augmentations (int): Number of augmented rows to create per original row.\n        batch_size (int): Number of augmented rows to process in a batch.\n\n    Yields:\n        pd.DataFrame: DataFrame containing a batch of augmented data.\n    \"\"\"\n    augmented_rows = []\n\n    for _, row in df.iterrows():\n        original_landmarks = np.array(row[\"landmarks\"])\n        \n        for _ in range(num_augmentations):\n            augmented_landmarks = augment_landmarks(original_landmarks)\n            new_row = {\n                \"class\": row[\"class\"],\n                \"landmarks\": augmented_landmarks.tolist(),\n                \"encoded_class\": row.get(\"encoded_class\", None)\n            }\n            augmented_rows.append(new_row)\n            \n            # Yield batch when it reaches the specified size\n            if len(augmented_rows) >= batch_size:\n                yield pd.DataFrame(augmented_rows)\n                augmented_rows = []\n    \n    # Yield remaining rows\n    if augmented_rows:\n        yield pd.DataFrame(augmented_rows)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T03:59:41.378154Z","iopub.execute_input":"2025-01-04T03:59:41.378390Z","iopub.status.idle":"2025-01-04T03:59:41.392520Z","shell.execute_reply.started":"2025-01-04T03:59:41.378370Z","shell.execute_reply":"2025-01-04T03:59:41.391802Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def save_augmented_data(df, num_augmentations=5, batch_size=1000, output_file=\"augmented_data.csv\"):\n    with open(output_file, 'w', newline='') as csvfile:\n        for idx, batch in enumerate(augment_dataset_in_batches(df, num_augmentations, batch_size)):\n            # Write header only for the first batch\n            batch.to_csv(csvfile, mode='a', header=(idx == 0), index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T03:59:41.393878Z","iopub.execute_input":"2025-01-04T03:59:41.394091Z","iopub.status.idle":"2025-01-04T03:59:41.405372Z","shell.execute_reply.started":"2025-01-04T03:59:41.394064Z","shell.execute_reply":"2025-01-04T03:59:41.404689Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"save_augmented_data(df, num_augmentations=50, batch_size=1000, output_file=\"augmented_data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T03:59:41.406116Z","iopub.execute_input":"2025-01-04T03:59:41.406343Z","execution_failed":"2025-01-04T04:12:23.706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define timesteps, frames, and features\ntimesteps = 50  # Time steps\nframes = 21  # Number of frames per time step\nfeatures = 3  # Number of features per frame","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the dataset into training and validation sets\ndf_train, df_val = train_test_split(df_org_augmented, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare X_train and y_train from the training set\nX_train = np.array([\n    np.array(landmark).reshape(50, 21, 6) \n    for landmark in df_train[\"landmarks\"]\n])\ny_train = df_train[\"encoded_class\"].values\n\n# Prepare X_val and y_val from the validation set\nX_val = np.array([\n    np.array(landmark).reshape(50, 21, 6) \n    for landmark in df_val[\"landmarks\"]\n])\ny_val = df_val[\"encoded_class\"].values\n\n# Verify the split and shapes\nprint(f\"Training set size: {len(X_train)}\")\nprint(f\"Validation set size: {len(X_val)}\")\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"X_val shape: {X_val.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Flatten the data to 2D (samples, timesteps * features)\nX_train_flattened = X_train.reshape(X_train.shape[0], -1)\nX_val_flattened = X_val.reshape(X_val.shape[0], -1)\n\n# Normalize landmarks (fit_transform on the training set, and transform on validation)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_flattened)\nX_val_scaled = scaler.transform(X_val_flattened)\n\n# Reshape the scaled data back to the original 3D shape (50, 21, 6)\nX_train = X_train_scaled.reshape(X_train.shape[0], 50, 21, 6)\nX_val = X_val_scaled.reshape(X_val.shape[0], 50, 21, 6)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the shapes after reshaping\nprint(f\"X_train shape after reshaping: {X_train.shape}\")\nprint(f\"X_val shape after reshaping: {X_val.shape}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert labels to one-hot encoding\nfrom keras.utils import to_categorical\ny_train_one_hot = to_categorical(y_train, num_classes)\ny_val_one_hot = to_categorical(y_val, num_classes)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Flatten the 3D input for ViT model (timesteps, features)\ntimesteps = 50  # Number of timesteps (frames)\nfeatures = 21 * 6  # Features per frame (21 landmarks * 6 values)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_vit = X_train.reshape(-1, timesteps, features)\nX_val_vit = X_val.reshape(-1, timesteps, features)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Vision Transformer (ViT) Model\ndef build_vit_model(input_shape, num_classes):\n    \"\"\"\n    Build a Vision Transformer (ViT) model for gesture classification.\n\n    Args:\n        input_shape (tuple): Shape of the input (timesteps, features).\n        num_classes (int): Number of gesture classes.\n\n    Returns:\n        keras.Model: Compiled model.\n    \"\"\"\n    input_layer = layers.Input(shape=input_shape)\n\n    # Add LayerNormalization\n    x = layers.LayerNormalization()(input_layer)\n\n    # Add Transformer block (Multi-Head Attention)\n    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Add()([input_layer, x])  # Skip connection\n\n    # Add Feed-forward Network\n    x = layers.LayerNormalization()(x)\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(256, activation='relu')(x)\n\n    # Add Global Average Pooling and Output Layer\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(128, activation='relu')(x)\n    output = layers.Dense(num_classes, activation='softmax')(x)\n\n    # Compile the model\n    model = models.Model(inputs=input_layer, outputs=output)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\n# Define input shape and build the ViT model\ninput_shape = (timesteps, features)\nmodel = build_vit_model(input_shape, num_classes)\n\n# Display model summary\nmodel.summary()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up the log directory for TensorBoard\nlog_dir = os.path.join(\"logs\", \"fit\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    X_train_vit,  # Reshaped training data\n    y_train_one_hot,  # One-hot encoded labels for training\n    validation_data=(X_val_vit, y_val_one_hot),  # Reshaped validation data and labels\n    epochs=50,  # Number of epochs\n    batch_size=8,  # Mini-batch size\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',  # Monitor validation loss\n            patience=5,  # Stop training if no improvement for 5 epochs\n            restore_best_weights=True  # Restore the best model weights\n        ),\n        tensorboard_callback  # TensorBoard callback\n    ],\n    verbose=1  # Display training progress\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training and validation loss\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.show()\n\n# Plot training and validation accuracy\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Assuming 'scaler' is your scaler object\nscaler_save_path = \"scaler.pkl\"\njoblib.dump(scaler, scaler_save_path)\nprint(f\"Scaler saved to: {scaler_save_path}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_save_path = \"my_vit_model.h5\"\n# Save the entire model\nmodel.save(model_save_path)\nprint(f\"Model saved to: {model_save_path}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the saved model\nloaded_model = tf.keras.models.load_model(model_save_path)\n\n# Verify the model structure\nloaded_model.summary()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-04T04:12:23.709Z"}},"outputs":[],"execution_count":null}]}