{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from datetime import datetime\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('new_landmarks/combined_dataset.csv')\n",
    "# df2 = pd.read_csv('hand_landmarks_from_4500_to_5000.csv')\n",
    "\n",
    "# df = pd.concat([df1, df2], ignore_index=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>landmarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>z</td>\n",
       "      <td>[[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>z</td>\n",
       "      <td>[[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>z</td>\n",
       "      <td>[[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>z</td>\n",
       "      <td>[[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>z</td>\n",
       "      <td>[[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class                                          landmarks\n",
       "2455     z  [[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...\n",
       "2456     z  [[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...\n",
       "2457     z  [[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...\n",
       "2458     z  [[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],...\n",
       "2459     z  [[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0],..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['class'] != 'to']\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, landmarks_column=\"landmarks\", class_column=\"class\"):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset for model training.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with class labels and landmarks columns.\n",
    "        landmarks_column (str): Name of the column containing landmarks.\n",
    "        class_column (str): Name of the column containing class labels.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with numeric landmarks and encoded labels.\n",
    "        LabelEncoder: Encoder for decoding class labels.\n",
    "    \"\"\"\n",
    "    # Ensure landmarks are in numeric format\n",
    "    df[landmarks_column] = df[landmarks_column].apply(\n",
    "        lambda x: np.array(ast.literal_eval(x)) if isinstance(x, str) else np.array(x)\n",
    "    )\n",
    "\n",
    "    # Encode class labels\n",
    "    le = LabelEncoder()\n",
    "    df[\"encoded_class\"] = le.fit_transform(df[class_column])\n",
    "\n",
    "    return df, le\n",
    "\n",
    "# Apply the preprocessing\n",
    "df, label_encoder = preprocess_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'about' 'aim' 'all' 'and' 'audio' 'b' 'barrier' 'break' 'c' 'can'\n",
      " 'communication' 'creative' 'd' 'detect' 'developed' 'e' 'f' 'g' 'h'\n",
      " 'have' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'our' 'p' 'project' 'q' 'r' 's'\n",
      " 'sign language' 'solution' 't' 'team' 'text' 'that' 'translate' 'u' 'v'\n",
      " 'w' 'what' 'x' 'y' 'you' 'z'] 49\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(label_encoder.classes_)\n",
    "print(label_encoder.classes_, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_landmarks(landmarks, max_noise=0.01, scale_range=(0.9, 1.1)):\n",
    "    \"\"\"\n",
    "    Apply random augmentations to landmarks.\n",
    "\n",
    "    Args:\n",
    "        landmarks (np.ndarray): Original landmarks array.\n",
    "        max_noise (float): Maximum noise to add to each coordinate.\n",
    "        scale_range (tuple): Range for random scaling factors.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Augmented landmarks.\n",
    "    \"\"\"\n",
    "    # Ensure landmarks are float64 for operations\n",
    "    landmarks = landmarks.astype(np.float64)\n",
    "    \n",
    "    # Add random noise\n",
    "    noise = np.random.uniform(-max_noise, max_noise, size=landmarks.shape)\n",
    "    landmarks += noise\n",
    "\n",
    "    # Apply random scaling\n",
    "    scale_factor = random.uniform(*scale_range)\n",
    "    landmarks *= scale_factor\n",
    "\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(df, num_augmentations=5):\n",
    "    \"\"\"\n",
    "    Augment the dataset by creating multiple augmented landmarks data for each row.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'class' and 'landmarks' columns.\n",
    "        num_augmentations (int): Number of augmented rows to create per original row.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with augmented data added.\n",
    "    \"\"\"\n",
    "    augmented_rows = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        original_landmarks = np.array(row[\"landmarks\"])\n",
    "        \n",
    "        # Generate multiple augmentations for each row\n",
    "        for _ in range(num_augmentations):\n",
    "            augmented_landmarks = augment_landmarks(original_landmarks)\n",
    "            \n",
    "            # Create a new row with the same class and augmented landmarks\n",
    "            new_row = {\n",
    "                \"class\": row[\"class\"],\n",
    "                \"landmarks\": augmented_landmarks.tolist(),  # Convert to list for DataFrame compatibility\n",
    "                \"encoded_class\": row.get(\"encoded_class\", None)  # Handle missing 'encoded_class'\n",
    "            }\n",
    "            augmented_rows.append(new_row)\n",
    "    \n",
    "    # Convert augmented rows to DataFrame\n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    \n",
    "    # Combine original and augmented data\n",
    "    combined_df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org_augmented = augment_dataset(df, num_augmentations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define timesteps, frames, and features\n",
    "timesteps = 50  # Time steps\n",
    "frames = 21  # Number of frames per time step\n",
    "features = 3  # Number of features per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "df_train, df_val = train_test_split(df_org_augmented, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 7712\n",
      "Validation set size: 1928\n",
      "X_train shape: (7712, 50, 21, 6)\n",
      "y_train shape: (7712,)\n",
      "X_val shape: (1928, 50, 21, 6)\n",
      "y_val shape: (1928,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare X_train and y_train from the training set\n",
    "X_train = np.array([\n",
    "    np.array(landmark).reshape(50, 21, 6) \n",
    "    for landmark in df_train[\"landmarks\"]\n",
    "])\n",
    "y_train = df_train[\"encoded_class\"].values\n",
    "\n",
    "# Prepare X_val and y_val from the validation set\n",
    "X_val = np.array([\n",
    "    np.array(landmark).reshape(50, 21, 6) \n",
    "    for landmark in df_val[\"landmarks\"]\n",
    "])\n",
    "y_val = df_val[\"encoded_class\"].values\n",
    "\n",
    "# Verify the split and shapes\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data to 2D (samples, timesteps * features)\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flattened = X_val.reshape(X_val.shape[0], -1)\n",
    "\n",
    "# Normalize landmarks (fit_transform on the training set, and transform on validation)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_flattened)\n",
    "X_val_scaled = scaler.transform(X_val_flattened)\n",
    "\n",
    "# Reshape the scaled data back to the original 3D shape (50, 21, 6)\n",
    "X_train = X_train_scaled.reshape(X_train.shape[0], 50, 21, 6)\n",
    "X_val = X_val_scaled.reshape(X_val.shape[0], 50, 21, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after reshaping: (7712, 50, 21, 6)\n",
      "X_val shape after reshaping: (1928, 50, 21, 6)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes after reshaping\n",
    "print(f\"X_train shape after reshaping: {X_train.shape}\")\n",
    "print(f\"X_val shape after reshaping: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "from keras.utils import to_categorical\n",
    "y_train_one_hot = to_categorical(y_train, num_classes)\n",
    "y_val_one_hot = to_categorical(y_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 3D input for ViT model (timesteps, features)\n",
    "timesteps = 50  # Number of timesteps (frames)\n",
    "features = 21 * 6  # Features per frame (21 landmarks * 6 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vit = X_train.reshape(-1, timesteps, features)\n",
    "X_val_vit = X_val.reshape(-1, timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">252</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">392,192</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,321</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m126\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m126\u001b[0m)             │             \u001b[38;5;34m252\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m392,192\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)                  │           \u001b[38;5;34m6,321\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">431,661</span> (1.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m431,661\u001b[0m (1.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">431,661</span> (1.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m431,661\u001b[0m (1.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mediapipe.python.solutions.hands import Hands\n",
    "\n",
    "def build_blazepose_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Build a model for gesture classification using BlazePose-based features.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input (timesteps, features).\n",
    "        num_classes (int): Number of gesture classes.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Compiled model.\n",
    "    \"\"\"\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Process BlazePose landmark features\n",
    "    x = layers.LayerNormalization()(input_layer)\n",
    "\n",
    "    # Add LSTM to capture temporal dependencies\n",
    "    x = layers.LSTM(256, return_sequences=False)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Fully connected layers for classification\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define input shape and number of classes\n",
    "timesteps = 50  # Example value, replace with actual\n",
    "features = 21 * 3 * 2  # Example: 21 landmarks with x, y, z coordinates\n",
    "\n",
    "input_shape = (timesteps, features)\n",
    "model = build_blazepose_model(input_shape, num_classes)\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the log directory for TensorBoard\n",
    "log_dir = os.path.join(\"logs\", \"fit\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m 45/964\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - accuracy: 0.0199 - loss: 3.9951    "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_vit,  # Reshaped training data\n",
    "    y_train_one_hot,  # One-hot encoded labels for training\n",
    "    validation_data=(X_val_vit, y_val_one_hot),  # Reshaped validation data and labels\n",
    "    epochs=50,  # Number of epochs\n",
    "    batch_size=8,  # Mini-batch size\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',  # Monitor validation loss\n",
    "            patience=5,  # Stop training if no improvement for 5 epochs\n",
    "            restore_best_weights=True  # Restore the best model weights\n",
    "        ),\n",
    "        tensorboard_callback  # TensorBoard callback\n",
    "    ],\n",
    "    verbose=1  # Display training progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'scaler' is your scaler object\n",
    "scaler_save_path = \"scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_save_path)\n",
    "print(f\"Scaler saved to: {scaler_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"my_vit_model.h5\"\n",
    "# Save the entire model\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = tf.keras.models.load_model(model_save_path)\n",
    "\n",
    "# Verify the model structure\n",
    "loaded_model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6420653,
     "sourceId": 10366272,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
